{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Зададим гиперпараметры модели"
      ],
      "metadata": {
        "id": "pDMxKRfr-G7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d5OXdQKZ91XS"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1 # Параметр эпсилон при использовании эпсилон жадной стратегии\n",
        "gamma = 0.9 # Коэффциент дисконтирования гамма\n",
        "random_seed = 5 #Random seed\n",
        "time_delay = 0.1 # Задержка времени при отрисовке процесса игры после обучения (секунды)\n",
        "lr_rate = 0.9 #Коэффициент скорости обучения альфа"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библиотеки, создаем свою среду размера 6х6. S обозначает точку старта. F -- лед безопасен, H -- проталина, G -- цель. Параметр is_slippery=False отвечает за условное отсутствие скольжения. То есть если агент выбрал действие пойти направо, то он переместится в соответствующее состояние. В общем случае из-за \"скольжения\" можно оказаться в другом состоянии. Мы также скопировали из библиотки GYM и слегка модифицировали функцию ```generate_random_map ```, для того, чтобы генерировать произвольные карты на основе ```random_seed ```."
      ],
      "metadata": {
        "id": "4kU_HA1j-Jer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gym==0.18.0\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def generate_random_map(size, p, sd):\n",
        "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
        "    :param size: size of each side of the grid\n",
        "    :param p: probability that a tile is frozen\n",
        "    \"\"\"\n",
        "    valid = False\n",
        "    np.random.seed(sd)\n",
        "\n",
        "    # DFS to check that it's a valid path.\n",
        "    def is_valid(res):\n",
        "        frontier, discovered = [], set()\n",
        "        frontier.append((0,0))\n",
        "        while frontier:\n",
        "            r, c = frontier.pop()\n",
        "            if not (r,c) in discovered:\n",
        "                discovered.add((r,c))\n",
        "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "                for x, y in directions:\n",
        "                    r_new = r + x\n",
        "                    c_new = c + y\n",
        "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
        "                        continue\n",
        "                    if res[r_new][c_new] == 'G':\n",
        "                        return True\n",
        "                    if (res[r_new][c_new] not in '#H'):\n",
        "                        frontier.append((r_new, c_new))\n",
        "        return False\n",
        "\n",
        "    while not valid:\n",
        "        p = min(1, p)\n",
        "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
        "        res[0][0] = 'S'\n",
        "        res[-1][-1] = 'G'\n",
        "        valid = is_valid(res)\n",
        "    return [\"\".join(x) for x in res]\n",
        "\n",
        "\n",
        "random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Создаем свою карту\n",
        "maze = random_map #Переменная maze служит для отрисовки маршрута агента\n",
        "env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Инициализируем среду\n",
        "print(\"Ваша карта\")\n",
        "env.render() #Выводим карту на экран"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONbkFonv-C4L",
        "outputId": "467e0649-5018-447c-b609-87c179956cf4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.18.0 in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.18.0) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.18.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow<=7.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.18.0) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.18.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from gym==0.18.0) (1.7.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.18.0) (0.16.0)\n",
            "Ваша карта\n",
            "\n",
            "\u001b[41mS\u001b[0mHFHFF\n",
            "FFFFFF\n",
            "FFHFFF\n",
            "FFFFFF\n",
            "FFFHHF\n",
            "FFFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции выбора действия и обновления таблицы ценности действий. Строчка *** используется для того, чтобы проверять ответы в openedx. Вне рамках академической задачи лучше использовать оригинальный метод класса `environment`, например:\n",
        "\n",
        "`action = env.action_space.sample()`\n",
        "\n",
        "# Задача 1\n",
        "Дополнить функцию ```learn()```, чтобы в результате ее вызова обновлялось значение ценности текущего действия согласно алгоритму Q-обучения"
      ],
      "metadata": {
        "id": "E1KR7uL8_tH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(state):\n",
        "    action=0\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = np.random.randint(0,env.action_space.n) #***\n",
        "    else:\n",
        "        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "def learn(state, state2, reward, action, done):\n",
        "    #Q-learning\n",
        "    if done:\n",
        "      Q[state, action] = Q[state, action] + lr_rate * (reward - Q[state, action])\n",
        "    else:\n",
        "      Q[state, action] = Q[state, action] + lr_rate * (reward + gamma * np.max(Q[state2, :]) - Q[state, action])"
      ],
      "metadata": {
        "id": "Mt-nhA51_ya1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 2\n",
        "Дополните следующий код так, чтобы в результате обучения модели можно было узнать количество побед и номер игры (`episode`), на котором агент впервые одержал пятую победу подряд.\n",
        "\n",
        "Поясним, что возвращает функция ```env.step(action)```\n",
        "\n",
        "```state2``` -- следующее состояние\n",
        "\n",
        "```reward``` -- награда\n",
        "\n",
        "```done``` -- условие окончания игры"
      ],
      "metadata": {
        "id": "w6S6GZfyACNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Inititalization\n",
        "wins_arr = [] #delete\n",
        "np.random.seed(random_seed)\n",
        "total_episodes = 10000\n",
        "max_steps = 100\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "min_episode = 0 #delete\n",
        "#Main cycle\n",
        "for episode in tqdm(range(total_episodes)):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    while t < max_steps:\n",
        "      #delete\n",
        "        if episode > 5 and wins_arr[episode-5] == 1 and wins_arr[episode-4] == 1 and wins_arr[episode-3] == 1 and wins_arr[episode-2] == 1 and wins_arr[episode-1] == 1 and min_episode ==0:\n",
        "          min_episode = episode\n",
        "        \n",
        "        t += 1\n",
        "\n",
        "        action = choose_action(state)\n",
        "\n",
        "        state2, reward, done, info = env.step(action)\n",
        "\n",
        "        if t == max_steps:\n",
        "          done = True  \n",
        "\n",
        "        learn(state, state2, reward, action, done)\n",
        "\n",
        "        state = state2\n",
        "\n",
        "\n",
        "        if done and reward == 1:\n",
        "          wins_arr.append(1) #record if won\n",
        "          break\n",
        "        if done:\n",
        "          wins_arr.append(0) #record if lost\n",
        "          break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QDAfTzbAETW",
        "outputId": "80851873-d363-4a3c-9964-19b5318c7d91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:08<00:00, 1192.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для заданных параметров при использовани алгоритма Q-обучения должны получиться следующие ответы:\n",
        "\n",
        "'Left': 0,\n",
        "    'Down': 1,\n",
        "    'Right': 2, \n",
        "    'Up': 3"
      ],
      "metadata": {
        "id": "dFfqNBnzAUXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Таблица ценностей действий\")\n",
        "#print(np.round(Q,4))\n",
        "#Number of wins\n",
        "print(\"Количество побед в серии из 10 000 игр: \", np.sum(wins_arr))\n",
        "#Number of the episode\n",
        "print(\"Пять побед подряд впервые было одержано в игре \",min_episode)\n",
        "#print(\"Q-table\")\n",
        "#print(np.round(Q,2))\n",
        "#print(\"Number of wins: \", #your code here)\n",
        "#print(\"Number of the episode\", #your code here)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5G1CSACAUyd",
        "outputId": "176bbf48-fc81-4261-f5fe-2b5eb1e22ca0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество побед в серии из 10 000 игр:  8010\n",
            "Пять побед подряд впервые было одержано в игре  825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Произведем одну игру, чтобы проследить за действиями агента. При этом будем считать модель полностью обученной, то есть действия выбираются жадно, значения ценностей действий в таблице не обновляются."
      ],
      "metadata": {
        "id": "RcRFb_jpAWnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#Жадный выбор действий\n",
        "def choose_action_one_game(state):\n",
        "    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "#Массив для сохранения состояний агента в течение игры\n",
        "states=[]\n",
        "t = 0\n",
        "state = env.reset()\n",
        "wn = 0\n",
        "while(t<100):\n",
        "  env.render()\n",
        "  time.sleep(time_delay)\n",
        "  clear_output(wait=True)\n",
        "  action = choose_action_one_game(state)  \n",
        "  state2, reward, done, info = env.step(action)  \n",
        "  states.append(state)\n",
        "  state = state2\n",
        "  t += 1\n",
        "  if done and reward == 1:\n",
        "    wn=1\n",
        "  if done:\n",
        "    break\n",
        "if wn == 1:\n",
        "  print(\"!!!Победа!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZCqzbjOAY3q",
        "outputId": "83ee5864-ad42-482b-d3b5-b76582c16997"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!Победа!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отобразим маршрут"
      ],
      "metadata": {
        "id": "Ar35ojZnAa1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_maze_pic(maze):\n",
        "  maze_pic=[]\n",
        "  for i in range(len(maze)):\n",
        "    row = []\n",
        "    for j in range(len(maze[i])):\n",
        "      if maze[i][j] == 'S':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'F':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'H':\n",
        "        row.append(1)\n",
        "      if maze[i][j] == 'G':\n",
        "        row.append(0)\n",
        "    maze_pic.append(row)\n",
        "  maze_pic = np.array(maze_pic)\n",
        "  return maze_pic\n",
        "  \n",
        "\n",
        "#Make maze fit to plot\n",
        "maze_pic = make_maze_pic(random_map)\n",
        "nrows, ncols = maze_pic.shape\n",
        "\n",
        "#Arrays of picture elements\n",
        "rw = np.remainder(states,nrows)\n",
        "cl = np.floor_divide(states,nrows)\n",
        "if wn == 1:\n",
        "  rw = np.append(rw, [nrows-1])\n",
        "  cl = np.append(cl,[ncols-1])\n",
        "\n",
        "#Picture plotting\n",
        "fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n",
        "ax1.clear()\n",
        "ax1.set_xticks(np.arange(0.5, nrows, step=1))\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_yticks(np.arange(0.5, ncols, step=1))\n",
        "ax1.set_yticklabels([])\n",
        "ax1.grid(True)\n",
        "ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n",
        "ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n",
        "ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n",
        "ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n",
        "ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n",
        "ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n",
        "ax1.imshow(maze_pic, cmap=\"binary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "UkrgcnGSAti3",
        "outputId": "3c63d7a3-fb46-40cb-8567-5ebafc568664"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7a2357b4c0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAP30lEQVR4nO3db2wc9Z3H8c/kD0ZrBxwB2ZAYe6FqTKsczd26bbiCbOseXEFE1z8PSrLiaEjZ/tHp5KZAr0S6QCWfejqfzg9AQrYqeMA2VnqI5i6o6nHHrkuRtqp9tYDoEq53iU2aP4UcBq+3MYn9uwdT42y8dtbJfD07m/dLGtnzm/FvPl55P54Zr23POScAsLAi7AAAahcFA8AMBQPADAUDwAwFA8DMqqXsfOONN7pEImEUJXgnTpzQyZMnw45RkZtvvlkbNmwIO0bFovTYStLtt9+u+vr6sGNUbHJyMlJ5h4eH33XO3TRvg3Ou4iWZTLoo6enpcZIisfT09IT9cC1JlB5bSS6bzYb9kC1J1PJKGnJlOmNJZzCz1ves1+nJ05fzocsiXh/XqUdOhR0DuOpd1j2Yai4XqfrzAVcLbvICMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwQAwE27BvL5d+qej0hPT/tvXt4caB0CwLusv2gXi9e3Sv/ZL5/7wd0ffT/jrknTHvtBiAQhOeGcw//F3c+Uy61y9Pw6gJoRXMO83L20cQOSEVzDXjy1tHEDkhFcwf/a4tHqydGz1pD8OoCaEVzB37JO2PSytPCvJSdcf89e5wQvUjMAL5nO3fE6vPfSaxr87rjOPndEvdv5CbRva9OCnHtSrO18t3fmOfVJTXmoZlL596yXLpeX6Frm9Tiu9lUHHBmAg0B9Tr7lmjQ7uOKhvvvRN7T+0X9esvEZ3N9+tqfNTVzw3pQJET6BnMJtu2CRJGnhzQDNuRmfPn9XL//uyzs2c0zP3PaM7m+7UxPcm9N5335Mk3fvxe/WfLyX1/ht3aaxrTHvb93401+zZykN//JBGu0b1yoOv6Oc7fy5JGv+bcU18b0Jbm7YGGR9AwAI9g3nrzFuanpnWc3/xnAYODSh/PK/xs+M6/O5hfePgN/S1P/ma7n727o/2n/xwUn+5+7AOvTWpzY/9tV5+4GWNnBrRgSMHPtqnvaVdn3j6E5pxM4rXx3Ws65gaf9CoaTcdZHQABgI9g5n4cEJ3PXuXnJz6t/XrnUff0YH7D2hd/bqy+w+ODurNI5NyTnrjd29o35v71J5oL9nnidwTKp4r6uz5s0FGBbAMAv9VgcPvHtbOAzslSa03tOr5Lz2v3j/v1c/+52fz9v3Mxs/oB/s+pc2b6nVN/bjqVtXpx4d+XLLP2x+8HXREAMvE9MfUR84c0XMjz2nzus1ycvO2/+hLP9K//PsZ3fKneTX+faOeGXpGnueV7OPc3MeVmwNA9Qq0YFpvaNXuO3dr45qNkqSm65q0ffN25X+b1+nCaTVd16TVK1Z/tP+aujX6v/Fzmpqa0ac3fFo7/mjHovO/M/mOpmemddva24KMDcBIoJdIEx9O6LMbP6vdW3er8dpGjZ8d18H/PqhH/+1RnT1/Vod+d0inHjmlGTejm/7hJn3rpW/pH7/9vJ568uMaPPG32n9ovxqvbVxw/t+f/726X+3Waw+9ptUrV+vzz39ev/ztL4P8FAAEKNCCOTFxQl/5568suP2+ffeVrL/wXy/ohcf+yl/Zua1k2+j7o/KeLL1ckqS9ub3am9s7bxxA9eEv2gEwQ8EAMEPBADBDwQAwQ8EAMEPBADBzWQUTr48HnSNQ1Z4PuFpc1utgTj1yKrAAHVn/bW4vvwYA1Brvwt/1KbuD56UlpSUpHo8nBwYGAg3Q1bVFktTbOxLovJJUKBTU0NAQ+LwWopRVIq+1qOXt7Owcds61zdvgnKt4SSaTLmjt7f5iIZvN2kxsIEpZnSOvtajllTTkynQGN3kBmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJgJtWAyGSmflwYHpUTCX69WmYyfccWK6s8KVIvL+qPfQchkpHRampry10dH/XVJSqXCSlXebNZi0V+v5qxANQmtYPbsmXvCzioWpV27pP7+YI4xPr5FjY1XPk8+P1eEs4pF/3OgYICFhXaJNDZWfvziJ3I1WCjTQp8DAF9oZzDNzf6lxsVaWqRcLphj5HIj6ujouOJ5EonyWZubr3hqoKaFdgbT3S3FYqVjsZg/Xm2ilBWoJqEVTCol9fX5Zyye57/t66vOexqzWevq/PVqzgpUk9AukST/CRqVJ2kqNXfzOahLOKDW8UI7AGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmPOfc4jt4XlpSWpLi8XhyYGBgOXIFolAoqKGhIbD5urq2SJJ6e0cCm3NWoVDQkSNHAp/XSmtra6CPrbWgvxasRS1vZ2fnsHOubd4G51zFSzKZdFGSzWYDna+93V8sZLNZJykyS9CPrTXy2pI05Mp0BpdIAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAVymSkfF4aHJQSCX8dwOIomApkMlI6LU1N+eujo/46JQMsjoKpwJ49UrFYOlYs+uMAFkbBVGBsbGnjAHwUTAWam5c2DsBHwVSgu1uKxUrHYjF/HMDCKJgKpFJSX59UV+evt7T466lUuLmAarcq7ABRkUpJ/f3++7lcqFGAyOAMBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCqaKOOciswCV8C71xeJ5XlpSWpLi8XhyYGBgOXIFolAoqKGhIbD5urq2SJJ6e0cCm3NW0FmtkddW1PJ2dnYOO+fa5m1YynetZDLpoiSbzQY6X3u7v1gIOqs18tqKWl5JQ65MZ3CJBMAMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAVToUxGyuelwUEpkfDXq1km4+dcsSIaeVGbVoUdIAoyGSmdlqam/PXRUX9dklKp8HItZDZvseivV3te1C4KpgJ79sw9WWcVi9KuXVJ/fzDHGB/fosbGYObK5+fKcFax6H8eFAyWE5dIFRgbKz9+8ZO4WiyUa6HPA7DCGUwFmpv9y4yLtbRIuVwwx8jlRtTR0RHIXIlE+bzNzYFMD1SMM5gKdHdLsVjpWCzmj1ejqOVF7aJgKpBKSX19/hmL5/lv+/qq937GbN66On+92vOidnGJVKFUKlpP0FRq7gZ0UJdxwFJxBgPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAUDwAwFA8AMBQPADAWDyzI8PCzP8yKzRC1vrfCcc4vv4HlpSWlJisfjyYGBgeXIFYhCoaCGhoawY1TEImtX1xZJUm/vSKDzStLp06d1/PjxwOe10tTUFKm8ra2tkfnalaTOzs5h51zbvA3OuYqXZDLpoiSbzYYdoWIWWdvb/cVCT0+PkxSZJWp5o/S165xzkoZcmc7gEgmAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKpkZlMlI+Lw0OSomEvw4sNwqmBmUyUjotTU3566Oj/jolg+VGwdSgPXukYrF0rFj0x4HlRMHUoLGxpY0DViiYGtTcvLRxwAoFU4O6u6VYrHQsFvPHgeVEwdSgVErq65Pq6vz1lhZ/PZUKNxeuPqvCDgAbqZTU3++/n8uFGgVXMc5gAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJihYACYoWAAmKFgAJjxnHOL7+B5aUlpSYrH48mBgYHlyBWIQqGghoaGsGNUxCJrV9cWSVJv70ig80rRemwl8lrr7Owcds61zdvgnKt4SSaTLkqy2WzYESpmkbW93V8sROmxdY681iQNuTKdwSUSUM3Wr5c8r3qX9esXjU/BANXs9OmwEyzuEvkoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYomBqVyUj5vDQ4KCUS/no1y2T8nCtWkNdCRtuV0FGt0LQSOqqMti/LcVcty1GwrDIZKZ2Wpqb89dFRf12SUqnwci1kNm+x6K+TN1gZbVda/SqqXpI0qoTS6pckpbTP9NiX/KPfF2pra3NDQ0OGcYKVy+XU0dERdoyKBJk1kfC/6C9WVydt3RrIITQ+Pq7GxsZA5srn58rwQuRtlAZzVzxXXls1pWvnjbfomI7p1iueX87J87yyf/SbS6QaNDZWfrzck6IaLJSLvMGYUl3Z8TE1mx+bS6Qa1Nxc/gympUXK5YI5Ri43Yn7GRd4Oyeu84rkSOqpRJeaNN2uB70QB4gymBnV3S7FY6Vgs5o9XI/La6tbjimmyZCymSXXrcfNjUzA1KJWS+vr876ie57/t66vOG5ASea/YxIR068L3UlLapz49rBYd08SE0123HlefHl74Bm97u/T224FE4xKpRqVS1fsELYe8FTp6VIrHpenpubFNm6STJxf9sJT2+YWyRnrVOOKFOIMBombbNmnNmrnlEuUSJgoGiDrnpI99zH//2Welp56SDh6UPvjA/5n6bbeV3/eee6RDh/z9jh+XvvOd0nl37/b/sdqJE9JXv3pZ0SgYoNbcf7/05JPS2rXSb36z8N3nH/5Q+vrXpeuukzZvll55ZW7b+vXS9ddLGzdKu3ZJTz8tXcbriCgYIGp+8hPpvff85cUX529/8UXpV7/y79NkMtKWLeXnOXdO+uQn/cus8XHp178u3fb970vnz0s//alUKEitrUuOSsEAUfOFL/hnJ2vXSl/84vztp07NvV8sSg0N5ef58pele+/1X9STy5W+DPnMmdIbyYvNswgKBrhaDQ35ZbVunX9WtH9/4IegYICr0erV0o4d/v2X8+f9G70zM4EfhtfBAFerBx7wf+K0cqV05IjJC3soGCBKyr1i1/Pm3t+5s3Tb4KB0yy3l973nnvLHuPhjFjpuBbhEAmCGggFghoIBYIaCAWCGggFghoIBYIaCAapZPB52gsVdIh+vgwGq2YW/VxRBlywYz/PSkv7wX19U8DzviG2kQN0o6d2wQ1QoSlkl8lqLWt6yv2q9pP+LFDWe5w2V+18t1ShKWSXyWquVvNyDAWCGggFgptYLpi/sAEsQpawSea3VRN6avgcDIFy1fgYDIEQUDAAzFAwAMxQMADMUDAAz/w9/hIvyqpUe2AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}